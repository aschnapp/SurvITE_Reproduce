{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aschnapp/SurvITE_Reproduce/blob/main/SurvITE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfhrg53y3U3G",
        "outputId": "c0cde0dd-2c98-44f0-9fcd-17a73d14ce83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.9/dist-packages (0.20.0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (1.24.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (1.2.0)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (2.0.12)\n",
            "Requirement already satisfied: scikit-learn<1.3,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (1.2.2)\n",
            "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (0.6.2.post0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from scikit-survival) (2.8.4)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.9/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival) (0.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->scikit-survival) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->scikit-survival) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<1.3,>=1.2.0->scikit-survival) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->scikit-survival) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.9/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from tf_slim) (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-survival\n",
        "!pip install tf_slim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyZzTKvk3Z2U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "def f_get_minibatch(mb_size, x1, x2=None, x3=None, x4=None, x5=None):\n",
        "    idx = range(np.shape(x1)[0])\n",
        "    idx = random.sample(idx, mb_size)\n",
        "    \n",
        "    if x2 is None:\n",
        "        return x1[idx].astype(float)\n",
        "    if x3 is None:\n",
        "        return x1[idx].astype(float), x2[idx].astype(float)\n",
        "    if x4 is None:\n",
        "        return x1[idx].astype(float), x2[idx].astype(float), x3[idx].astype(float)\n",
        "    if x5 is None:\n",
        "        return x1[idx].astype(float), x2[idx].astype(float), x3[idx].astype(float), x4[idx].astype(float)\n",
        "    \n",
        "    return x1[idx].astype(float), x2[idx].astype(float), x3[idx].astype(float), x4[idx].astype(float), x5[idx].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuucWY5H3k7_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_EPSILON = 1e-08\n",
        "\n",
        "\n",
        "################################\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def log(x):\n",
        "    return tf.log(x + _EPSILON)\n",
        "\n",
        "def div(x, y):\n",
        "    return tf.compat.v1.div(x, (y + _EPSILON))\n",
        "\n",
        "################################\n",
        "##### IPM TERMS\n",
        "def pdist2sq(X,Y):\n",
        "    \"\"\" Computes the squared Euclidean distance between all pairs x in X, y in Y \"\"\"\n",
        "    C = -2*tf.matmul(X,tf.transpose(Y))\n",
        "    nx = tf.compat.v1.reduce_sum(tf.square(X),1,keep_dims=True)\n",
        "    ny = tf.compat.v1.reduce_sum(tf.square(Y),1,keep_dims=True)\n",
        "    D = (C + tf.transpose(ny)) + nx\n",
        "    return D\n",
        "\n",
        "\n",
        "def mmd2_lin(X1,X2,W1=None,W2=None,p=0.5,weights=None):\n",
        "    ''' Linear MMD '''    \n",
        "    if (W1 is None) and (W2 is None):\n",
        "        W1 = tf.ones_like(X1[:,0])\n",
        "        W2 = tf.ones_like(X2[:,0])\n",
        "    \n",
        "    W1     = div(W1, tf.compat.v1.reduce_sum(W1))\n",
        "    W2     = div(W2, tf.compat.v1.reduce_sum(W2))\n",
        "    \n",
        "    W1     = tf.reshape(W1, [-1,1])\n",
        "    W2     = tf.reshape(W2, [-1,1])\n",
        "        \n",
        "    mean1 = tf.compat.v1.reduce_sum(W1*X1, axis=0)\n",
        "    mean2 = tf.compat.v1.reduce_sum(W2*X2, axis=0)\n",
        "    \n",
        "    mmd = tf.compat.v1.reduce_sum(tf.square(2.0*p*mean1 - 2.0*(1.0-p)*mean2))\n",
        "    \n",
        "    return mmd\n",
        "\n",
        "\n",
        "def wasserstein(X1,X2,W1=None,W2=None,p=0.5,lam=10,its=10): #,sq=False,backpropT=False):\n",
        "    \"\"\" Returns the Wasserstein distance between treatment groups \"\"\"    \n",
        "    n1 = tf.compat.v1.to_float(tf.shape(X1)[0])\n",
        "    n2 = tf.compat.v1.to_float(tf.shape(X2)[0])\n",
        "    \n",
        "    ''' Compute distance matrix'''\n",
        "    M = pdist2sq(X1,X2)\n",
        "        \n",
        "    #for now consider W1 and W2 is [None,] shape\n",
        "    if (W1 is None) and (W2 is None):\n",
        "        W1 = tf.ones_like(X1[:,0])\n",
        "        W2 = tf.ones_like(X2[:,0])\n",
        "    \n",
        "    W1     = div(W1, tf.compat.v1.reduce_sum(W1))\n",
        "    W2     = div(W2, tf.compat.v1.reduce_sum(W2))\n",
        "    \n",
        "    W1     = tf.reshape(W1, [-1,1])\n",
        "    W2     = tf.reshape(W2, [-1,1])\n",
        "    W_mask = tf.tile(W1, [1, n2]) * tf.tile(tf.transpose(W2), [n1, 1])\n",
        "    \n",
        "    ''' Estimate lambda and delta '''\n",
        "    M_mean = tf.compat.v1.reduce_sum(M*W_mask) #this becomes weighted average\n",
        "    \n",
        "    M_drop  = tf.nn.dropout(M, 10/(n1*n2))\n",
        "    delta   = tf.stop_gradient(tf.reduce_max(M))\n",
        "    eff_lam = tf.stop_gradient(lam/M_mean)\n",
        "\n",
        "    ''' Compute new distance matrix '''\n",
        "    Mt  = M\n",
        "    row = delta*tf.ones(tf.shape(M[0:1,:]))\n",
        "    col = tf.concat([delta*tf.ones(tf.shape(M[:,0:1])),tf.zeros((1,1))], axis=0)\n",
        "    Mt  = tf.concat([M,row], axis=0)\n",
        "    Mt  = tf.concat([Mt,col], axis=1)\n",
        "\n",
        "    ''' Compute marginal vectors '''        \n",
        "    a = tf.concat([p*tf.ones_like(X1[:,0:1])*W1, (1-p)*tf.ones((1,1))], axis=0)\n",
        "    b = tf.concat([(1-p)*tf.ones_like(X2[:,0:1])*W2, p*tf.ones((1,1))], axis=0)\n",
        "\n",
        "    ''' Compute kernel matrix'''\n",
        "    Mlam = eff_lam*Mt\n",
        "    K = tf.exp(-Mlam) + 1e-6 # added constant to avoid nan\n",
        "    U = K*Mt\n",
        "    ainvK = K/a\n",
        "\n",
        "    u = a\n",
        "    for i in range(0,its):\n",
        "        u = 1.0/(tf.matmul(ainvK,(b/tf.transpose(tf.matmul(tf.transpose(u),K)))))\n",
        "    v = b/(tf.transpose(tf.matmul(tf.transpose(u),K)))\n",
        "\n",
        "    T = u*(tf.transpose(v)*K)\n",
        "\n",
        "    E = T*Mt\n",
        "    D = 2*tf.compat.v1.reduce_sum(E)\n",
        "\n",
        "    return D #, Mlam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-4JtyZB3gA9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tf_slim as slim\n",
        "\n",
        "_EPSILON = 1e-08\n",
        "\n",
        "################################\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def log(x):\n",
        "    return tf.log(x + _EPSILON)\n",
        "\n",
        "def div(x, y):\n",
        "    return tf.compat.v1.div(x, (y + _EPSILON))\n",
        "\n",
        "\n",
        "##### NETWORK FUNCTIONS\n",
        "# removed reuse=tf.AUTO_REUSE\n",
        "def fcnet(x_, o_dim_, o_fn_, num_layers_=1, h_dim_=100, activation_fn=tf.nn.relu, keep_prob_=1.0, w_reg_=None, name='fcnet'):\n",
        "    '''\n",
        "        x_            : (2D-tensor) input\n",
        "        o_dim_        : (int) output dimension\n",
        "        o_type_       : (string) output type one of {'continuous', 'categorical', 'binary'}\n",
        "        num_layers_   : (int) # of hidden layers\n",
        "        activation_fn_: tf activation functions\n",
        "        reuse         : (bool) \n",
        "    '''\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "        if num_layers_ == 1:\n",
        "            out =  slim.layers.fully_connected(inputs=x_, num_outputs=o_dim_, activation_fn=o_fn_, weights_regularizer=w_reg_, scope='layer_out')\n",
        "        else:\n",
        "            for tmp_layer in range(num_layers_-1):\n",
        "                if tmp_layer == 0:\n",
        "                    net = x_\n",
        "                net = slim.layers.fully_connected(inputs=net, num_outputs=h_dim_, activation_fn=activation_fn, weights_regularizer=w_reg_, scope='layer_'+str(tmp_layer))\n",
        "                net = tf.compat.v1.nn.dropout(net, keep_prob=keep_prob_)\n",
        "            out =  slim.layers.fully_connected(inputs=net, num_outputs=o_dim_, activation_fn=o_fn_, weights_regularizer=w_reg_, scope='layer_out')  \n",
        "    return out\n",
        "\n",
        "\n",
        "################################\n",
        "##### NETWORK \n",
        "class SurvITE:\n",
        "    def __init__(self, sess, name, input_dims, network_settings):\n",
        "        self.sess               = sess\n",
        "        self.name               = name\n",
        "\n",
        "        ### INPUT DIMENSIONS\n",
        "        self.x_dim              = input_dims['x_dim']\n",
        "        self.t_max              = input_dims['t_max']\n",
        "        self.num_Event          = input_dims['num_Event'] #Without counting censoring.\n",
        "        \n",
        "\n",
        "        ### NETWORK HYPER-PARMETERS\n",
        "        self.z_dim              = network_settings['z_dim']  #PHI(X)\n",
        "        \n",
        "        self.h_dim1             = network_settings['h_dim1']  #PHI\n",
        "        self.h_dim2             = network_settings['h_dim2']  #Hypothesis\n",
        "        \n",
        "        self.num_layers1        = network_settings['num_layers1']\n",
        "        self.num_layers2        = network_settings['num_layers2']\n",
        " \n",
        "        self.active_fn          = network_settings['active_fn']\n",
        "        self.reg_scale          = network_settings['reg_scale']\n",
        "        \n",
        "        self.ipm_term           = network_settings['ipm_term']\n",
        "        self.is_treat           = network_settings['is_treat'] #boolean\n",
        "        self.is_smoothing       = network_settings['is_smoothing'] #boolean\n",
        "        \n",
        "        assert self.ipm_term in ['mmd_lin', 'wasserstein', 'no_ipm']\n",
        "\n",
        "        self.clipping_thres     = 10.\n",
        "\n",
        "        \n",
        "        self._build_net()\n",
        "\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.compat.v1.variable_scope(self.name):\n",
        "            #### PLACEHOLDER DECLARATION\n",
        "            self.lr_rate        = tf.compat.v1.placeholder(tf.float32, [], name='learning_rate')\n",
        "            self.k_prob         = tf.compat.v1.placeholder(tf.float32, [], name='keep_probability')   #keeping rate\n",
        "            self.alpha          = tf.compat.v1.placeholder(tf.float32, [], name='alpha')\n",
        "            self.beta           = tf.compat.v1.placeholder(tf.float32, [], name='beta')\n",
        "            self.gamma          = tf.compat.v1.placeholder(tf.float32, [], name='gamma')\n",
        "\n",
        "            self.x              = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x_dim], name='inputs')\n",
        "            self.y              = tf.compat.v1.placeholder(tf.float32, shape=[None, self.num_Event], name='labels')   #event/censoring label (censoring: the last column)\n",
        "            self.t              = tf.compat.v1.placeholder(tf.float32, shape=[None, 1], name='times')\n",
        "            self.a              = tf.compat.v1.placeholder(tf.float32, shape=[None, 1], name='treatment_assignments')\n",
        "            self.w              = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_max, 2], name='weights')            \n",
        "            \n",
        "            self.is_training    = tf.compat.v1.placeholder(tf.bool, name = 'train_test_indicator') #for batch_normalization\n",
        "            \n",
        "            self.mb_size        = tf.shape(self.x)[0]\n",
        "            \n",
        "            ### mask generation -- for easier computation for at-risk patients\n",
        "            tmp_range      = tf.cast(tf.expand_dims(tf.range(0, self.t_max, 1), axis=0), tf.float32)\n",
        "            self.mask1     = tf.cast(tf.equal(tmp_range, self.t), tf.float32)\n",
        "            self.mask2     = tf.cast(tf.less_equal(tmp_range, self.t), tf.float32)\n",
        "\n",
        "            y_expanded     = self.mask1 * self.y\n",
        "            \n",
        "            \n",
        "            #PHI(x)\n",
        "            self.z              = fcnet(\n",
        "                x_=self.x, o_dim_=self.z_dim, o_fn_=None, \n",
        "                num_layers_=self.num_layers1, h_dim_=self.h_dim1, activation_fn=self.active_fn, \n",
        "                keep_prob_=self.k_prob, name='encoder'\n",
        "            )\n",
        "            \n",
        "            \n",
        "            ###BATCH NORMALIZATION. (This follows the implementation of CFRNet)\n",
        "#             self.z = tf.math.l2_normalize(self.z, axis=0)\n",
        "            self.z = tf.compat.v1.layers.batch_normalization(self.z, training=self.is_training)\n",
        "            self.z = self.active_fn(self.z)\n",
        "            self.z = tf.compat.v1.nn.dropout(self.z, keep_prob=self.k_prob)\n",
        "\n",
        "            def tmpfcnetgen(name):\n",
        "                return fcnet(\n",
        "                    x_=self.z, o_dim_=1, o_fn_=None, \n",
        "                    num_layers_=self.num_layers2, h_dim_=self.h_dim2, activation_fn=self.active_fn, \n",
        "                    keep_prob_=self.k_prob, name=name+'{}'.format(m)\n",
        "                )\n",
        "            ### H(Z; A,T)\n",
        "            for m in range(self.t_max):\n",
        "                tmp_A1              = tmpfcnetgen('hypothesis_A1_T')\n",
        "                \n",
        "                if self.is_treat:\n",
        "                    tmp_A0              = tmpfcnetgen('hypothesis_A0_T')\n",
        "                else:\n",
        "                    tmp_A0              = tf.zeros_like(tmp_A1)\n",
        "                    \n",
        "                if m == 0:\n",
        "                    self.logits_A1 = tmp_A1\n",
        "                    self.logits_A0 = tmp_A0\n",
        "                else:\n",
        "                    self.logits_A1 = tf.concat([self.logits_A1, tmp_A1], axis=1)\n",
        "                    self.logits_A0 = tf.concat([self.logits_A0, tmp_A0], axis=1)\n",
        "                    \n",
        "            \n",
        "            ### loss - IPM regularization\n",
        "            self.loss_IPM1 = 0. #treated\n",
        "            self.loss_IPM0 = 0. #not-treated\n",
        "\n",
        "            self.w_clipped = tf.clip_by_value(self.w, 0., self.clipping_thres, name='weights_clipped')\n",
        "            def gentfcond(idx, fn, clippedidx):\n",
        "                return tf.cond(tf.equal(tf.size(idx), 0),\n",
        "                    lambda: tf.constant(0, tf.float32),\n",
        "                    lambda: fn(self.z, \n",
        "                                    tf.gather(self.z, idx, axis=0), \n",
        "                                    tf.ones_like(self.z[:,0]), \n",
        "                                    tf.gather(self.w_clipped[:, m, clippedidx], idx, axis=0))\n",
        "                )\n",
        "            if self.ipm_term != 'no_ipm':\n",
        "                # for m in range(1, self.t_max):\n",
        "                for m in range(0, self.t_max):\n",
        "                    idx1             = tf.where(tf.equal(self.a[:, 0]*self.mask2[:, m], 1.))[:,0]\n",
        "                     \n",
        "                    if self.is_treat:\n",
        "                        idx0             = tf.where(tf.equal((1.-self.a[:, 0])*self.mask2[:, m], 1.))[:,0]\n",
        "                    \n",
        "                    \n",
        "                    if self.ipm_term == 'mmd_lin':\n",
        "                        self.loss_IPM1 += gentfcond(idx1, mmd2_lin, 0)\n",
        "                        if self.is_treat:\n",
        "                            self.loss_IPM0 += gentfcond(idx0, mmd2_lin, 1)\n",
        "                    elif self.ipm_term == 'wasserstein':\n",
        "                        self.loss_IPM1 += gentfcond(idx1, wasserstein, 0)\n",
        "                        if self.is_treat:\n",
        "                            self.loss_IPM0 += gentfcond(idx0, wasserstein, 1)\n",
        "            self.loss_IPM = self.loss_IPM1 + self.loss_IPM0\n",
        "\n",
        "\n",
        "\n",
        "            ### loss - smoothing regularization\n",
        "            self.loss_smoothing_A1 = 0. #treated\n",
        "            self.loss_smoothing_A0 = 0. #not-treated\n",
        "\n",
        "\n",
        "            if self.is_smoothing:\n",
        "                for m in range(1, self.t_max):\n",
        "                    tmp_Wprev = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name+'/hypothesis_A1_T{}'.format(m-1))[::2]\n",
        "                    tmp_Wcurr = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name+'/hypothesis_A1_T{}'.format(m))[::2]\n",
        "                    for l in range(self.num_layers2):\n",
        "                        self.loss_smoothing_A1 += tf.reduce_mean((tmp_Wprev[l] - tmp_Wcurr[l])**2) ## average over each parameter (for scaling)\n",
        "                    \n",
        "                    if self.is_treat:\n",
        "                        tmp_Wprev = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name+'/hypothesis_A0_T{}'.format(m-1))[::2]\n",
        "                        tmp_Wcurr = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name+'/hypothesis_A0_T{}'.format(m))[::2]\n",
        "                        for l in range(self.num_layers2):\n",
        "                            self.loss_smoothing_A0 += tf.reduce_mean((tmp_Wprev[l] - tmp_Wcurr[l])**2) ## average over each parameter (for scaling)\n",
        "            \n",
        "            self.loss_smoothing = self.loss_smoothing_A1 + self.loss_smoothing_A0             \n",
        "                    \n",
        "            tmp_w1 = div(self.w[:, :, 0], tf.reduce_sum(self.mask2 * self.a * self.w[:, :, 0], axis=0, keepdims=True) )\n",
        "            tmp_w1 = self.mask2 * self.a * tmp_w1\n",
        "            \n",
        "            if self.is_treat:\n",
        "                tmp_w0 = div(self.w[:, :, 1], tf.reduce_sum(self.mask2 * (1.- self.a) * self.w[:, :, 1], axis=0, keepdims=True) )\n",
        "                tmp_w0 = self.mask2 * (1. - self.a) * tmp_w0\n",
        "                            \n",
        "            ### loss - factual loss\n",
        "            self.loss      = 0\n",
        "            loss_A1        = tf.reduce_sum(\n",
        "                tmp_w1 * self.mask2 * self.a * tf.nn.sigmoid_cross_entropy_with_logits(labels=y_expanded, logits=self.logits_A1)\n",
        "            )\n",
        "            self.loss     += loss_A1\n",
        "            if self.is_treat:\n",
        "                loss_A0        = tf.reduce_sum(\n",
        "                    tmp_w0 * self.mask2 * (1.- self.a) * tf.nn.sigmoid_cross_entropy_with_logits(labels=y_expanded, logits=self.logits_A0)\n",
        "                )            \n",
        "                self.loss     += loss_A0            \n",
        "                   \n",
        "            ### l2-regularization    \n",
        "            self.vars_encoder = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name+'/encoder')\n",
        "            \n",
        "            if self.reg_scale != 0:\n",
        "                vars_reg          = [w for w in self.vars_encoder if 'weights' in w.name]\n",
        "                regularizer       = slim.layers.l2_regularizer(scale=self.reg_scale, scope=None)\n",
        "                loss_reg          = slim.layers.apply_regularization(regularizer, vars_reg)   \n",
        "            else:\n",
        "                loss_reg          = 0.\n",
        "\n",
        "\n",
        "            self.solver       = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.loss)\n",
        "            \n",
        "            self.loss_total   = self.loss + self.beta * self.loss_IPM + self.gamma * self.loss_smoothing + loss_reg\n",
        "            self.solver_total = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.loss_total)\n",
        "            \n",
        "            \n",
        "            ### batch-normalization operation\n",
        "            self.extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS) \n",
        "                      \n",
        "                    \n",
        "    def predict_hazard(self, x_, logits): \n",
        "        odd    = tf.exp(logits)\n",
        "        hazard = odd / (1. + odd)\n",
        "        return self.sess.run(hazard, feed_dict={self.x:x_, self.k_prob:1.0, self.is_training: False})\n",
        "\n",
        "    def predict_hazard_A1(self, x_):\n",
        "        return self.predict_hazard(x_, self.logits_A1)\n",
        "         \n",
        "    def predict_hazard_A0(self, x_):\n",
        "        return self.predict_hazard(x_, self.logits_A0)\n",
        "    \n",
        "    def predict_survival(self, x_, logits):\n",
        "        hazard         = self.predict_hazard(x_, logits)  \n",
        "        surv           = np.ones_like(hazard)\n",
        "#         surv[:, 1:]    = np.cumprod(1. - hazard, axis=1)[:, :-1]\n",
        "        surv[:, :]    = np.cumprod(1. - hazard, axis=1)\n",
        "        return surv\n",
        "\n",
        "    def predict_survival_A1(self, x_):\n",
        "        return self.predict_survival(x_, self.logits_A1)\n",
        "    \n",
        "    def predict_survival_A0(self, x_):\n",
        "        return self.predict_survival(x_, self.logits_A0)\n",
        "        \n",
        "    def train_baseline(self, x_, y_, t_, a_, lr_train_=1e-3, k_prob_=1.0):\n",
        "        return self.sess.run([self.solver, self.extra_update_ops, self.loss],\n",
        "                             feed_dict={self.x:x_, self.y:y_, self.t:t_, self.a:a_, self.w:np.ones([np.shape(x_)[0], self.t_max, 2]),\n",
        "                                        self.lr_rate:lr_train_, \n",
        "                                        self.k_prob:k_prob_,\n",
        "                                        self.is_training: True})\n",
        "\n",
        "    def get_loss_basline(self, x_, y_, t_, a_, k_prob_=1.0):\n",
        "        return self.sess.run(self.loss,\n",
        "                             feed_dict={self.x:x_, self.y:y_, self.t:t_, self.a:a_, self.w:np.ones([np.shape(x_)[0], self.t_max, 2]),\n",
        "                                        self.k_prob:k_prob_,\n",
        "                                        self.is_training: False})\n",
        "    \n",
        "    \n",
        "    def train(self, x_, y_, t_, a_, w_, beta_=1e-3, gamma_=1e-3, lr_train_=1e-3, k_prob_=1.0):\n",
        "        if not self.is_smoothing:\n",
        "            gamma_ = 0.\n",
        "        return self.sess.run([self.solver_total, self.extra_update_ops, self.loss_total, self.loss, self.loss_IPM],\n",
        "                             feed_dict={self.x:x_, self.y:y_, self.t:t_, self.a:a_, self.w:w_,\n",
        "                                        self.beta:beta_, self.gamma:gamma_,\n",
        "                                        self.lr_rate:lr_train_, \n",
        "                                        self.k_prob:k_prob_,\n",
        "                                        self.is_training: True})\n",
        "\n",
        "    def get_loss(self, x_, y_, t_, a_, w_, beta_=1e-3, gamma_=1e-3, k_prob_=1.0):\n",
        "        if not self.is_smoothing:\n",
        "            gamma_ = 0.\n",
        "        return self.sess.run([self.loss_total, self.loss, self.loss_IPM],\n",
        "                             feed_dict={self.x:x_, self.y:y_, self.t:t_, self.a:a_, self.w:w_,\n",
        "                                        self.beta:beta_, self.gamma:gamma_,\n",
        "                                        self.k_prob:k_prob_,\n",
        "                                        self.is_training: False})\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIG-qaok2E5O"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sksurv.metrics import concordance_index_ipcw\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swE7rKwf24uy"
      },
      "outputs": [],
      "source": [
        "ipm_type      = 'wasserstein'\n",
        "\n",
        "weight        = False\n",
        "is_smoothing  = False\n",
        "is_treat      = True\n",
        "\n",
        "is_training   = False#True\n",
        "\n",
        "OUT_ITERATION = 5\n",
        "N_tr = 5000\n",
        "N_te = 5000\n",
        "\n",
        "if not weight:\n",
        "    weight_type = 'noweight'\n",
        "else:\n",
        "    weight_type  = '' \n",
        "\n",
        "\n",
        "if ipm_type == 'no_ipm':\n",
        "    beta       = 0.\n",
        "else:\n",
        "    beta   = 1e-3 #1e-3\n",
        "\n",
        "    \n",
        "if is_smoothing:\n",
        "    gamma  = 1e-3\n",
        "else:\n",
        "    gamma  = 0.\n",
        "\n",
        "\n",
        "lr_rate   = 1e-3\n",
        "mb_size   = 512\n",
        "\n",
        "keep_prob = 0.7\n",
        "\n",
        "seed           = 1234\n",
        "\n",
        "TMAX           = 30\n",
        "eval_times     = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLPRBZKN44bt"
      },
      "outputs": [],
      "source": [
        "seed         = 1234\n",
        "modelname    = 'SurvITE'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lVg1av06sZL"
      },
      "source": [
        "# Import Observational Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQlqQlf1493T"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "download data\n",
        "use this to get data from github, just replace URL.\n",
        "\"\"\"\n",
        "import requests\n",
        "import io\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/chl8856/survITE/d7f217fb2bc2406ec11656be49c4fc29156372fb/data/tr_data.npz')\n",
        "response.raise_for_status()\n",
        "npz = np.load(io.BytesIO(response.content))\n",
        "tr_x = npz['x']\n",
        "tr_a = npz['a']\n",
        "tr_t = npz['t']\n",
        "tr_y = npz['y']\n",
        "\n",
        "response_te = requests.get('https://raw.githubusercontent.com/chl8856/survITE/d7f217fb2bc2406ec11656be49c4fc29156372fb/data/te_data.npz')\n",
        "response_te.raise_for_status()\n",
        "npz = np.load(io.BytesIO(response_te.content))\n",
        "te_x = npz['x']\n",
        "te_a = npz['a']\n",
        "te_t = npz['t']\n",
        "te_y = npz['y']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSL5c4gK47Sy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Use this if you have local data\n",
        "\"\"\"\n",
        "# train_npz  = np.load('/content/s_data/data/S1/train.npz')\n",
        "# tr_x = train_npz['x']\n",
        "# tr_a = train_npz['a']\n",
        "# tr_t = train_npz['t']\n",
        "# tr_y = train_npz['y']\n",
        "\n",
        "# test_npz  = np.load('/content/s_data/data/S1/test.npz')\n",
        "# te_x = test_npz['x']\n",
        "# te_a = test_npz['a']\n",
        "# te_t = test_npz['t']\n",
        "# te_y = test_npz['y']\n",
        "\n",
        "tr_y_structured = [(tr_y[i], tr_t[i]) for i in range(len(tr_y))]\n",
        "tr_y_structured = np.array(tr_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
        "\n",
        "te_y_structured = [(te_y[i], te_t[i]) for i in range(len(te_y))]\n",
        "te_y_structured = np.array(te_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
        "\n",
        "TMAX           = 30\n",
        "eval_times     = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBGqFF5L6zu5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va2lnbvC6y26",
        "outputId": "8f26443b-8444-475c-c4a0-5bbc3a216f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ]
        }
      ],
      "source": [
        "num_Event   = 1\n",
        "\n",
        "z_dim       = 100\n",
        "x_dim       = np.shape(tr_x)[1]\n",
        "\n",
        "num_layers1  = 3\n",
        "h_dim1       = 100\n",
        "\n",
        "num_layers2  = 2\n",
        "h_dim2       = 100\n",
        "\n",
        "\n",
        "input_dims = {\n",
        "    'x_dim': x_dim,\n",
        "    'num_Event': num_Event,\n",
        "    't_max': TMAX+1\n",
        "}\n",
        "network_settings = {\n",
        "    'z_dim': z_dim,     \n",
        "\n",
        "    # Phi()\n",
        "    'h_dim1': h_dim1, \n",
        "    'num_layers1': num_layers1, \n",
        "\n",
        "    # Hypothesis()\n",
        "    'h_dim2': h_dim2, \n",
        "    'num_layers2': num_layers2,\n",
        "\n",
        "    'active_fn': tf.nn.elu,\n",
        "    'reg_scale': 0.,\n",
        "    'ipm_term' : ipm_type, \n",
        "    'is_treat' : is_treat,\n",
        "    'is_smoothing': is_smoothing\n",
        "}\n",
        "\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "model = SurvITE(sess, 'SurvITE', input_dims, network_settings)\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "saver       = tf.compat.v1.train.Saver()\n",
        "\n",
        "savepath = './{}/surviTE/'.format(modelname)\n",
        "\n",
        "if not os.path.exists(savepath):\n",
        "    os.makedirs(savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgBhMdqh65_H"
      },
      "outputs": [],
      "source": [
        "if not is_treat:\n",
        "    tr_a = np.ones_like(tr_t)\n",
        "    te_a = np.ones_like(te_t)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tr_y = tr_y.reshape([-1,1])\n",
        "tr_t = tr_t.reshape([-1,1])\n",
        "tr_a = tr_a.reshape([-1,1])\n",
        "\n",
        "if is_treat:\n",
        "    tr_w  = np.ones([np.shape(tr_x)[0], TMAX+1, 2])\n",
        "else:\n",
        "    tr_w  = np.ones([np.shape(tr_x)[0], TMAX+1, 1])\n",
        "\n",
        "tr_x_,va_x, tr_y_,va_y, tr_t_,va_t, tr_a_,va_a, tr_w_,va_w = train_test_split(tr_x, tr_y, tr_t, tr_a, tr_w, test_size=0.2, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEX8lh_F6-nY"
      },
      "outputs": [],
      "source": [
        "iterations = 20000\n",
        "\n",
        "avg_tr_loss_total = 0.\n",
        "avg_tr_loss       = 0.\n",
        "avg_tr_loss_ipm   = 0.\n",
        "\n",
        "avg_va_loss_total = 0.\n",
        "avg_va_loss       = 0.\n",
        "avg_va_loss_ipm   = 0.\n",
        "\n",
        "check_step        = 100\n",
        "\n",
        "min_loss          = 1e+8\n",
        "max_flag          = 20\n",
        "stop_flag         = 0\n",
        "\n",
        "IS_TRAIN = False\n",
        "\n",
        "if is_training:\n",
        "    for itr in range(iterations):    \n",
        "        if beta == 0.:\n",
        "            x_mb, y_mb, t_mb, a_mb = f_get_minibatch(mb_size, tr_x_, tr_y_, tr_t_, tr_a_)    \n",
        "            _, _, tmp_tr_loss          = model.train_baseline(x_mb, y_mb, t_mb, a_mb, lr_train_=lr_rate, k_prob_=keep_prob)\n",
        "            avg_tr_loss_total         += tmp_tr_loss/check_step\n",
        "            avg_tr_loss               += tmp_tr_loss/check_step\n",
        "\n",
        "            x_mb, y_mb, t_mb, a_mb = f_get_minibatch(min(mb_size, np.shape(va_x)[0]), va_x, va_y, va_t, va_a)    \n",
        "            tmp_va_loss                = model.get_loss_basline(x_mb, y_mb, t_mb, a_mb, k_prob_=keep_prob)\n",
        "            avg_va_loss_total         += tmp_va_loss/check_step\n",
        "            avg_va_loss               += tmp_va_loss/check_step\n",
        "\n",
        "        else:\n",
        "            x_mb, y_mb, t_mb, a_mb, w_mb = f_get_minibatch(mb_size, tr_x_, tr_y_, tr_t_, tr_a_, tr_w_)\n",
        "            _, _, tmp_tr_loss_total, tmp_tr_loss, tmp_tr_loss_ipm = model.train(x_mb, y_mb, t_mb, a_mb, w_mb, \n",
        "                                                                                beta_=beta, gamma_=gamma, \n",
        "                                                                                lr_train_=lr_rate, k_prob_=keep_prob)\n",
        "            avg_tr_loss_total         += tmp_tr_loss_total/check_step\n",
        "            avg_tr_loss               += tmp_tr_loss/check_step\n",
        "            avg_tr_loss_ipm           += tmp_tr_loss_ipm/check_step\n",
        "\n",
        "            x_mb, y_mb, t_mb, a_mb, w_mb = f_get_minibatch(min(mb_size, np.shape(va_x)[0]), va_x, va_y, va_t, va_a, va_w)    \n",
        "            tmp_va_loss_total, tmp_va_loss, tmp_va_loss_ipm      = model.get_loss(x_mb, y_mb, t_mb, a_mb, w_mb, \n",
        "                                                                                  beta_=beta, gamma_=gamma, \n",
        "                                                                                  k_prob_=1.0)\n",
        "            avg_va_loss_total         += tmp_va_loss_total/check_step\n",
        "            avg_va_loss               += tmp_va_loss/check_step\n",
        "            avg_va_loss_ipm           += tmp_va_loss_ipm/check_step\n",
        "\n",
        "        if (itr + 1)%check_step == 0:\n",
        "            stop_flag += 1\n",
        "\n",
        "            print(\n",
        "                \"ITR {:04d}  | TR: loss_T={:.3f} loss_S={:.3f} loss_IPM={:.3f} | loss_T={:.3f} loss_S={:.3f} loss_IPM={:.3f}\".format(\n",
        "                itr+1, avg_tr_loss_total, avg_tr_loss, avg_tr_loss_ipm, avg_va_loss_total, avg_va_loss, avg_va_loss_ipm)\n",
        "            )\n",
        "\n",
        "            if min_loss > avg_va_loss_total:\n",
        "                min_loss  = avg_va_loss_total\n",
        "                stop_flag = 0\n",
        "\n",
        "                saver.save(sess, savepath + 'model_{}{}'.format(ipm_type,weight_type))\n",
        "                print('model saved...')\n",
        "\n",
        "            else:\n",
        "                if stop_flag >= max_flag:\n",
        "                    break\n",
        "\n",
        "\n",
        "            avg_tr_loss_total = 0.\n",
        "            avg_tr_loss = 0.\n",
        "            avg_tr_loss_ipm = 0.\n",
        "\n",
        "            avg_va_loss_total = 0.\n",
        "            avg_va_loss = 0.\n",
        "            avg_va_loss_ipm = 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23M2sM5u7BS2"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naEcVvA71XjV",
        "outputId": "ad37086b-dde5-4aa2-d101-4eae77849e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Deep Learning/trained_models/SurvITE_S1.zip\n",
            "   creating: SurvITE/\n",
            "   creating: SurvITE/surviTE/\n",
            "  inflating: SurvITE/surviTE/model_wassersteinnoweight.index  \n",
            "  inflating: SurvITE/surviTE/model_wassersteinnoweight.meta  \n",
            "  inflating: SurvITE/surviTE/model_wassersteinnoweight.data-00000-of-00001  \n",
            "  inflating: SurvITE/surviTE/checkpoint  \n"
          ]
        }
      ],
      "source": [
        "#call this if restoring data\n",
        "if not IS_TRAIN:\n",
        "  !rm -rf SurvITE\n",
        "  !unzip '/content/drive/MyDrive/Deep Learning/trained_models/SurvITE_S1.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgHolDB7AYr"
      },
      "outputs": [],
      "source": [
        "saver.restore(sess, savepath + 'model_{}{}'.format(ipm_type,weight_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBhcDWII7Fe2"
      },
      "outputs": [],
      "source": [
        "surv1 = model.predict_survival_A1(te_x)\n",
        "surv0 = model.predict_survival_A0(te_x)\n",
        "\n",
        "hzrd1 = model.predict_hazard_A1(te_x)\n",
        "hzrd0 = model.predict_hazard_A0(te_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "564wVwWJjMSr"
      },
      "outputs": [],
      "source": [
        "# print(te_x.shape)\n",
        "np.savez('/content/drive/MyDrive/Deep Learning/Results/SurvITE/S1_results.npz', \n",
        "           surv_pred_0 = surv0, \n",
        "           surv_pred_1 = surv1,\n",
        "           haz_pred_0 = hzrd0,\n",
        "           haz_pred_1 = hzrd1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omde0VQrFDet"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}